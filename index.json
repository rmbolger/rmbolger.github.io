[{
    "title": "Hacking Infoblox Discovery for Fun and Profit",
    "date": "",
    "description": "It's Just Another CSV",
    "body": "An IPAM solution is not very useful unless it accurately reflects the state of your network. The discovery features in NIOS have always been one of the great benefits of having Infoblox DDI. Network Insight and NetMRI focus on discovery of network gear and what is connected to it. The vDiscovery feature adds the ability to discover virtual infrastructure in both public and private cloud environments. All of these working together can give you a super accurate picture of what exists on your network\u0026hellip;if all your gear is supported.\nAs much as it may seem like it, the discovery features are not magic. Infoblox engineers had to figure out how each supported vendor\u0026rsquo;s device works and how to query its details. New vendors, new models, and new firmware versions can all affect Infoblox\u0026rsquo;s ability to properly discover your network gear. In many cases, customers are the ones driving Infoblox to make additions to the support list. But what do you do if you have an unsupported device and can\u0026rsquo;t (or don\u0026rsquo;t want to) wait for the formal device support request process?\nYou could just do it yourself. üòè\nUsing NetMRI\u0026rsquo;s Example Because NetMRI is not part of the NIOS grid, it is a good model for figuring out how to upload our own discovery data from something external. The problem is we don\u0026rsquo;t really know how it works under the hood. But we have hints from the Audit Log on the grid master.\n\r\r\rEvery time NetMRI submits new discovery data, it calls a DataUploadInit function followed by SetDiscoveryCSV. After searching the WAPI documentation, SetDiscoveryCSV appears to be a standard WAPI function tied to the fileop object type. This is great because anything that can use the WAPI via REST can theoretically submit new discovery data. You can view the WAPI doc on your own grid using the following URL and substitue your own grid master host/IP, https://gm/wapidoc/objects/fileop.html#upload-setdiscoverycsv. Here is a screenshot from my lab GM\u0026rsquo;s copy.\n\r\r\rNow we have to figure out how to format our CSV properly and we should be good to go.\nSupport Bundle To The Rescue When I was first looking into this, I was afraid I\u0026rsquo;d need to do something like MitM the connection between NetMRI and the grid master in order to capture the CSV file being uploaded while it was in transit. Thankfully, that was unnecessary due to Infoblox\u0026rsquo;s support bundle functionality.\nIf you have ever created an Infoblox support ticket, you have likely also been asked to generate and attach a support bundle to the ticket. It\u0026rsquo;s a tar/gzipped file containing a bunch of low-level diagnostic stuff from the appliance it was generated on (log files, config files, etc). It is also a great resource for better understanding Infoblox internals or even trying to solve your own support issues if you have the time and patience.\nWhile poking around in my lab GM\u0026rsquo;s support bundle, I noticed a folder called /storage/discovery/last_csv_import which looked very promising. It contained another numbered folder likely tied to an internal ID of some sort and within that was a file called import_file.\n\r\r\rAfter extracting the file, it appeared to be a complete copy of the file NetMRI had submitted with all column headers intact. üéâ Hooray! üéâ\nCSV Format Here is a list of all the headers included in the CSV file we pulled from the support bundle:\n ip_address ap_ip_address ap_name ap_ssid bgp_as bridge_domain device_contact device_location device_model device_vendor discovered_name discoverer endpoint_groups first_discovered_timestamp last_discovered_timestamp mac_address netbios_name network_component_contact network_component_description network_component_ip network_component_location network_component_model network_component_name network_component_port_description network_component_port_name network_component_port_number network_component_type network_component_vendor os port_duplex port_link_status port_speed port_status port_vlan_name port_vlan_number tenant vrf_description vrf_name vrf_rd task_name  And here are the contents of the CSV I used as my initial test file. (Stargate fan. Sorry, not sorry)\nip_address,device_location,device_model,device_vendor,discovered_name,discoverer,last_discovered_timestamp,task_name \u0026#34;192.168.0.9\u0026#34;,\u0026#34;Dakara\u0026#34;,\u0026#34;Stargate\u0026#34;,\u0026#34;Altera\u0026#34;,\u0026#34;sg01\u0026#34;,\u0026#34;MyDiscoveryScript\u0026#34;,\u0026#34;2020-09-14 21:34:18\u0026#34;,\u0026#34;Stargate_Checks\u0026#34; Most of these fields correspond to the fields in the discoverydata struct which you can view in your GM\u0026rsquo;s WAPI documentation (https://gm/wapidoc/additional/structs.html#struct-discoverydata). But there are a few important differences.\nip_address is mandatory in the CSV file but not listed in the struct. The struct has first_discovered and last_discovered which are Epoch seconds, but the CSV uses first_discovered_timestamp and last_discovered_timestamp which are UTC timestamps in yyyy-MM-dd HH:mm:ss format. Everything else seems to be a 1:1 match with the discoverydata struct. So, you can likely add and use additional columns NetMRI doesn\u0026rsquo;t currently send like fields related to vDiscovery.\nThe other thing I noticed about the discovery timestamp values is you only need to include last_discovered_timestamp if you are planning to merge the discovery data with existing data. Infoblox will automatically set the first_discovered value based on the last_discovered_timestamp the first time you import data for a given IP. If you choose to replace the data instead, you will need to supply both values or else it will clear the existing first_discovered value.\nA Word About Permissions The NetMRI documentation does not really go into detail about the minimum required permissions for the Infoblox user it authenticates to the grid master with. While you could use an existing superuser account, it is not recommended from a security standpoint. Ideally, you would have a dedicated user for discovery data submission. After some trial and error, the minimum permissions you need are:\n Global - Grid Permissions - Network Discovery - Read/Write Object - \u0026lt;the network view object\u0026gt; - Read-Only  If you are targeting more than a single network view, you can either add each one individually or just give the read access for \u0026ldquo;All Network Views\u0026rdquo;. Here is a screenshot of the resulting permissions in the GUI.\n\r\r\rHere is an example of how to create the group, user, and permissions using Posh-IBWAPI.\n$groupName = \u0026#39;DiscoveryUsers\u0026#39; New-IBObject admingroup -IBObject @{name=$groupName; access_method=@(\u0026#39;API\u0026#39;)} New-IBObject adminuser -IBObject @{name=\u0026#39;disco-user1\u0026#39;; password=\u0026#39;\u0026lt;the password\u0026gt;\u0026#39;; admin_groups=@($groupName)} New-IBObject permission -IBObject @{group=$groupName; permission=\u0026#39;WRITE\u0026#39;; resource_type=\u0026#39;NETWORK_DISCOVERY\u0026#39;} # For adding read to a specific network view $viewRef = (Get-IBObject networkview -Filter \u0026#39;name=default\u0026#39;).\u0026#39;_ref\u0026#39; New-IBObject permission -IBObject @{group=$groupName; permission=\u0026#39;READ\u0026#39;; object=$viewRef} # For adding read to all network views New-IBObject permission -IBObject @{group=$groupName; permission=\u0026#39;READ\u0026#39;; resource_type=\u0026#39;NETWORK_VIEW\u0026#39;} Performing A Test Run With our CSV file ready to go, we can run the following via Posh-IBWAPI using its super awesome Send-IBFile function. (This assumes you\u0026rsquo;ve already setup a profile with your discovery user using Set-IBConfig)\nSend-IBFile setdiscoverycsv .\\discovery.csv -FunctionArgs @{merge_data=$true; network_view=\u0026#39;default\u0026#39;} If you can\u0026rsquo;t or don\u0026rsquo;t want to use Posh-IBWAPI, it\u0026rsquo;s a little more cumbersome because you have to make 3 separate REST calls to follow the standard file upload process with the WAPI. Here\u0026rsquo;s an example using curl on linux. (If you\u0026rsquo;re using curl from Windows, you\u0026rsquo;ll need to escape the JSON differently in the third command)\n# This call should return JSON containing \u0026#34;token\u0026#34; and \u0026#34;url\u0026#34; values. # The url must replace the example URL in the the next command # The token must replace the token value in the third command (don\u0026#39;t include # the trailing \\n if it exists) curl -k1 -u admin:infoblox -X POST \u0026#34;https://gm/wapi/v2.10/fileop?_function=uploadinit\u0026#34; # Replace the url using the value returned by the first command curl -k1 -u admin:infoblox -F name=discovery.csv -F filedata=@discovery.csv \u0026#34;https://192.168.0.2/http_direct_file_io/req_id-UPLOAD-0000000000000001/import_file\u0026#34; # Replace the token using the value returned by the first command curl -k1 -u admin:infoblox -X POST \u0026#34;https://gm/wapi/v2.10/fileop?_function=setdiscoverycsv\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;token\u0026#34;:\u0026#34;xxxxxxxxxxxx\u0026#34;, \u0026#34;merge_data\u0026#34;:true, \u0026#34;network_view\u0026#34;:\u0026#34;default\u0026#34;}\u0026#39; Unfortunately, the WAPI does not return anything useful to indicate the discovery data was processed successfully. You will only get an error if there was a functional problem with the REST call itself. If things worked, you should see your discovery data in the IPAM view for the IP address associated with your discovery data. Mine looks like this from our sample data. (Don\u0026rsquo;t forget you can customize which fields are shown using the little gear icon in the upper right)\n\r\r\rTroubleshooting If your WAPI call to SetDiscoveryCSV appears to have completed successfully and you do not see your test data in IPAM for the associated IP, the discovery engine may have had a problem parsing your CSV for some reason. The good news is there is a log file that should be able to tell you what is wrong. The bad news is it is buried in the support bundle. Generate and download a fresh one and then look in /infoblox/var/discovery_csv_error for the associated log file.\n\r\r\rNOTE: My lab grid is on NIOS 8.5.1. Readers have noted separate discovery logs may not exist on previous NIOS versions. If you don\u0026rsquo;t see them, check the main infoblox.log file for lines containing \u0026ldquo;discovery_gm_sorter\u0026rdquo;.\nWrapping Up We\u0026rsquo;ve nailed down everything we need to do in order to submit new discovery data. All that remains is writing the tool/script to query our unsupported devices and generate the appropriate CSV file. Now go forth and discover!\n",
    "ref": "/blog/2020/09/hacking-infoblox-discovery-for-fun-and-profit/"
  },{
    "title": "Using Let's Encrypt for Active Directory Domain Controller Certificates",
    "date": "",
    "description": "LDAPS for free without needing internal PKI.",
    "body": "If you\u0026rsquo;ve ever had to setup an HTTPS website in the past couple years, you\u0026rsquo;ve most likely heard of Let\u0026rsquo;s Encrypt which is arguably the largest public certificate authority in the world. Not only are their certificates free, the entire ordering and renewal process can be completely automated using a recently finalized protocol standard known as ACME (RFC 8555).\n\r\r\rBut there are endless apps and services other than HTTP based web sites that can also use TLS certificates. On Windows specifically, there are things like Remote Desktop (RDP), SQL Server, WinRM, Exchange, and Active Directory. Many folks don\u0026rsquo;t realize that the certificates you get from Let\u0026rsquo;s Encrypt can be used for these other services as well and they don\u0026rsquo;t even need to be exposed to the Internet as long as the domain name is real and obtained from a public domain registrar. This can be a huge operational win for smaller organizations who don\u0026rsquo;t have the resources or expertise to design, implement, and maintain an internal Public Key Infrastructure (PKI).\nActive Directory is historically a bit more picky about certs than some other services and ever since I wrote Posh-ACME, I\u0026rsquo;ve been curious if Let\u0026rsquo;s Encrypt certs would work with it. As it turns out, they work great with a couple minor caveats.\nCertificate Transparency and Security Through Obscurity \r\r\rAll publicly trusted CAs in 2019 including Let\u0026rsquo;s Encrypt must now adhere to the Certificate Transparency standard which is a \u0026ldquo;cryptographically assured, publicly auditable, append-only record\u0026rdquo; of issued certificates. In other words, all public certificates are now logged and searchable by the general public. So if your security team demands that your internal server or domain names remain a secret, you simply can\u0026rsquo;t use any public CA for internal certificates. Be sure to get approval from those folks before continuing with this if you have any doubt.\nEven if you can\u0026rsquo;t or won\u0026rsquo;t use public certs for your production AD, they\u0026rsquo;re still super handy for dev/test/lab domains.\nUsing Public Certs for Internal Services In order to get a certificate from a public CA like Let\u0026rsquo;s Encrypt, the FQDN in the cert must be part of a domain that was obtained from an ICANN recognized domain registrar. If your internal domains end in TLDs like .local or .int, you\u0026rsquo;re out of luck. You\u0026rsquo;re also more likely to run into future problems like everyone who was using .dev until Google purchased it. So if you\u0026rsquo;re doing that, stop it.\nIt might seem obvious, but the CA will also need you to prove control/ownership of the domain or names in your certificate. Generally, this means being able to modify the public DNS records for it. For Let\u0026rsquo;s Encrypt and any other ACME capable CA, you need the ability to create TXT records.\nLet\u0026rsquo;s say your domain is example.com and your internal service is on a machine with the FQDN dc.ad.example.com. In order to get a certificate for that name, you would have to create a TXT record called _acme-challenge.dc.ad.example.com with a special value in the public facing DNS zone for example.com. Any machine outside your network should be able to do a standard DNS query like this and have the special value returned.\nC:\\\u0026gt;nslookup -q=TXT _acme-challenge.dc.ad.example.com. Finally, the machine running the ACME client must be able to reach the ACME server on the Internet. If your environment blocks outbound Internet access, you\u0026rsquo;ll need to either move the ACME client to a whitelisted host or add exceptions for the ACME server URLs to your outbound firewall or proxy server. For Let\u0026rsquo;s Encrypt, that would be acme-v02.api.letsencrypt.org for the Production endpoint and acme-staging-v02.api.letsencrypt.org for the Staging endpoint. Having both available is very useful for troubleshooting.\nActive Directory and Certificates Adding TLS certificates to your Active Directory domain controllers has been a recommended practice for a long while now. One of the primary benefits is enabling LDAPS (LDAP over SSL) which prevents exposing cleartext credentials on the wire for legacy applications who still need to use basic BINDs. This will become increasingly important in 2020 when Microsoft changes the LDAP defaults to require channel binding and signing. Even for clients who use more modern BIND methods like Kerberos with SASL, it will protect the confidentiality of the LDAP query traffic which standard LDAP does not.\nHere is Microsoft\u0026rsquo;s official guidance on obtaining domain controller certificates from a third-party CA and enabling LDAP over SSL.\n Requirements for domain controller certificates from a third-party CA How to enable LDAP over SSL with a third-party certification authority  There are two main things we care about from those docs:\n Each DC\u0026rsquo;s cert must contain its own FQDN (dc.example.com) and the domain\u0026rsquo;s FQDN (example.com). The cert should be installed in the local computer\u0026rsquo;s Personal certificate store  Domain Controller Prep For this demo, we\u0026rsquo;ll be using a freshly installed Windows Server 2019 domain controller, dcle, in a domain called ad.poshacme.online. Server 2019 comes pre-installed with the necessary Posh-ACME prerequisites. But if you\u0026rsquo;re on an earlier OS, make sure you have PowerShell 5.1 and .NET 4.7.1 or later. Posh-ACME has been installed into the default system-wide module path, C:\\Program Files\\WindowsPowerShell\\Modules. See the readme for help with installation.\nWe\u0026rsquo;re going to run Posh-ACME in the context of the local SYSTEM account so it has permission to modify the local computer\u0026rsquo;s certificate store and restart services. In practice, any administrative account will do. Just make sure to run your PowerShell session elevated.\nUsing PsExec, open PowerShell as NT AUTHORITY\\SYSTEM and verify your context using whoami and $env:USERPROFILE.\n.\\PsExec64.exe -i -h -s powershell.exe \r\r\r\rGetting the Certificate If you\u0026rsquo;re not familiar with how Posh-ACME works, I\u0026rsquo;d suggest going through the tutorial first. This guide will assume you\u0026rsquo;re already familiar with the basics of getting a cert using a DNS plugin.\nI currently host the public facing DNS zone for this poshacme.online domain at Digital Ocean, so I\u0026rsquo;ll be using the DOcean DNS plugin for this order. Any plugin will work, but the Manual plugin will prevent automatic renewal because it requires human interaction.\nPrep the plugin arguments for the DNS plugin, generate the list of names we\u0026rsquo;ll need in the cert, and set the email address for expiration notification.\n# Digital Ocean requires a simple API token $pArgs = @{DOToken=\u0026#39;xxxxxxxxxxxx\u0026#39;} # The ActiveDirectory PowerShell module is installed by default on DCs $dc = Get-ADDomainController $env:COMPUTERNAME $certNames = @($dc.HostName, $dc.Domain) # This is optional, but usually a good idea. $notifyEmail = \u0026#39;myaddress@poshacme.online\u0026#39; Build a hashtable of parameters and use them with New-PACertificate via splatting. Make sure you include the -Install parameter so the resulting certificate gets installed to the local computer\u0026rsquo;s certificate store.\n$certParams = @{ Domain = $certNames DnsPlugin = \u0026#39;DOcean\u0026#39; PluginArgs = $pArgs AcceptTOS = $true Install = $true Contact = $notifyEmail # optional Verbose = $true # optional } New-PACertificate @certParams If all went well, you should see the new cert details in the PowerShell output and the certificate should show up in the Local Computer\\Personal certificate snap-in. Open it by running certlm.msc.\n\r\r\r\r\r\rVerifying LDAPS and ADWS The nice thing about domain controller certs is that LDAPS should immediately be functional with no service restarts. But how do we know it\u0026rsquo;s working?\nIf the DC did not previously have a cert, the Directory Service event log should contain an Event ID 1221 confirming LDAPS is now working. However, no events are generated on a cert renewal or replacement.\n\r\r\rAnother good way to test is using the native ldp.exe utility. Run it and select Connection - Connect specifying the name of your DC, 636 as the port, and check the SSL box. Then press OK. You should see some successful connection details and then a bunch of LDAP info from the RootDSE object.\n\r\r\rIf you want to see the specific details of the certificate being used, there are scripts such as this one that will attempt a connection to your DC and parse those details from the TLS conversation.\nIn addition to LDAPS, Active Directory Web Services (ADWS) will also use this new certificate. The PowerShell ActiveDirectory module (among other things) uses this service rather than raw LDAP to communicate with AD. However, it requires a service restart to recognize and use the new certificate. You can do that via the Services MMC snap-in or run Restart-Service ADWS. When done, you should find Event ID 1401 in the associated event log which confirms it has successfully loaded the certificate.\n\r\r\rExpiration and Renewal Let\u0026rsquo;s Encrypt certificates are only valid for 90 days. While you could manually repeat this process shortly before your cert expires every 70-80 days, it\u0026rsquo;s much less hassle to setup a scheduled task that will renew the certificate automatically. If you configured a notification email with the certificate order, Let\u0026rsquo;s Encrypt will email you if the cert hasn\u0026rsquo;t been renewed starting about 20 days before it expires. So once you have the renewal process automated, you can largely forget about it.\nPosh-ACME\u0026rsquo;s Submit-Renewal is designed to be run on a regular (daily) basis. It will only act when the suggested renewal window has been reached for a certificate and it will return the details for the new certificate if successful. There are a few things we want to accomplish in our scheduled task.\n Run Submit-Renewal If we got a new cert:  Delete the old certificate Restart the ADWS service   Log everything for future diagnostic purposes  Assuming no other Posh-ACME certificates have been ordered on this machine, our renewal script might look something like this.\nStart-Transcript $env:LOCALAPPDATA\\cert-renewal.log -Append $hostname = (Get-ADDomainController $env:COMPUTERNAME).HostName $oldCert = Get-ChildItem Cert:\\LocalMachine\\My | Where-Object { $_.Subject -eq \u0026#34;CN=$hostname\u0026#34; } | Sort-Object -Descending NotAfter | Select-Object -First 1 if (Submit-Renewal -Verbose) { $oldCert | Remove-Item Restart-Service ADWS } Stop-Transcript Now we\u0026rsquo;ll throw a slightly minified version of that code into a scheduled task and we\u0026rsquo;re good to go.\n$taskname = \u0026#34;Renew DC Certificate\u0026#34; $taskdesc = \u0026#34;Renews the Let\u0026#39;s Encrypt certificate installed on this domain controller.\u0026#34; $actionArg = \u0026#39;-C \u0026#34;Start-Transcript $env:LOCALAPPDATA\\cert-renewal.log -Append; $name=(Get-ADDomainController $env:COMPUTERNAME).HostName; $oldCert=gci Cert:\\LocalMachine\\My | ?{ $_.Subject -eq \\\u0026#34;CN=$name\\\u0026#34; } | sort -d NotAfter | select -f 1; if (Submit-Renewal -Verbose) { $oldCert | ri; Restart-Service ADWS } Stop-Transcript; exit $LASTEXITCODE\u0026#34;\u0026#39; $action = New-ScheduledTaskAction -Execute \u0026#39;powershell.exe\u0026#39; -Argument $actionArg $trigger = New-ScheduledTaskTrigger -Daily -At 2am -RandomDelay (New-TimeSpan -Minutes 30) $settings = New-ScheduledTaskSettingsSet -ExecutionTimeLimit (New-TimeSpan -Minutes 30) Register-ScheduledTask $taskname -Action $action -Trigger $trigger -User \u0026#39;System\u0026#39; -Settings $settings -Desc $taskdesc After running your new task, you should find the cert-renewal.log in C:\\Windows\\System32\\config\\systemprofile\\AppData\\Local unless you customized the path in the Start-Transcript call.\n\r\r\rThe transcript logging is optional. Some may want to skip it in favor of sending an email instead when things successfully renew or add additional error handling and only send an email if there\u0026rsquo;s an error. It\u0026rsquo;s all just PowerShell, so the possibilities are endless.\nWrapping Up I realize this solution is probably not realistic for an environment with more than a handful of DCs. Large environments are better suited to spin up a real PKI solution and use manually installed certs with multi-year expirations or native Windows auto-enrollment if the internal CA is Microsoft based. An internal CA also has the benefit of being able to use alternative name fields like IP address and GUID which are useful for certain edge cases.\nHowever, there are certainly ways to scale this if you really wanted to. And the resource and maintenance benefits of not having to run an internal CA are huge for small teams. I wouldn\u0026rsquo;t be surprised if there were small orgs already doing this with traditional purchased certificates.\nRegardless, I had a lot of fun exploring this topic and hope it shed some light on the versatility of Posh-ACME and alternative use cases for Let\u0026rsquo;s Encrypt certificates. Let\u0026rsquo;s Encrypt is an amazing organization that is truly making the web a more secure place for everyone. Consider becoming a donor or sponsor if you\u0026rsquo;d like to help them with that goal.\n",
    "ref": "/blog/2019/12/using-lets-encrypt-for-active-directory-domain-controller-certificates/"
  },{
    "title": "Posh-ACME 3.12.0",
    "date": "",
    "description": "Set-PAOrder improvements, upgraded BouncyCastle, and misc fixes.",
    "body": "Just shipped a new Posh-ACME release, version 3.12.0. The Set-PAOrder function now has -DnsPlugin and -PluginArgs parameters which should make it easier to change DNS providers and/or provider parameters without needing to wait for a certificate renewal. The BouncyCastle library has been updated to the latest 1.8.5 version and the DLL file is using a non-standard name to avoid conflicts with other software that uses BouncyCastle and chooses to install the DLL into the .NET GAC. For some reason, PowerShell will always choose to use the GAC copy rather than the module\u0026rsquo;s copy even if the module\u0026rsquo;s version is newer when they are named the same.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Set-PAOrder now has -DnsPlugin and -PluginArgs parameters to allow changing plugins and associated credentials prior to a renewal operation. Upgraded BouncyCastle library to version 1.8.5.2 and renamed the DLL to avoid conflicts with older copies that may get installed into the .NET GAC by other software. ACME server errors returned during calls to Revoke-PAAuthorization are now non-terminating errors rather than warnings. Fixed bug where new orders created with New-PACertificate and no explicit plugin wouldn\u0026rsquo;t get the Manual default if the account was already authorized for the included names. Fixed Get-PAAuthorizations when using explicit account reference Fixed datetime parsing issues on non-US culture environments (#208) Fixed errors thrown by Submit-Renewal when run against an order with a null DnsPlugin. A warning is now thrown instead. Fixed parameter binding error when using -PluginArgs with Submit-Renewal Fixed HurricanElectric guide\u0026rsquo;s parameter references Fixed Azure tests  ",
    "ref": "/blog/2019/12/posh-acme-3.12.0/"
  },{
    "title": "Posh-ACME 3.11.0",
    "date": "",
    "description": "Improvements for Install-PACertificate and new function Revoke-PAAuthorization.",
    "body": "Just shipped a new Posh-ACME release, version 3.11.0. The Install-PACertificate function now has optional parameters that allow you to specify the Windows certificate store location and name in case the defaults (LocalMachine\\My) aren\u0026rsquo;t what you need. You can also use the -NotExportable switch to mark the certificate as non-exportable. There\u0026rsquo;s also a new function called Revoke-PAAuthorization which is mostly useful for testing a new configuration. It allows you to revoke one or more existing authorizations associated with an account so that when you generate a new certificate, the ACME server will require a full re-validation for those names.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Added Revoke-PAAuthorization which enables revocation of identifier authorizations associated with an account. Get-PAAuthorizations now has an optional -Account parameter and better error handling. Get-PAAuthorization has been added as an alias for Get-PAAuthorizations to better comply with PowerShell naming standards. It will likely be formally renamed in version 4.x and the old name should be considered deprecated. This change should allow dependent scripts to prepare for that change in advance. Install-PACertificate now supports parameters to select the store name, location, and the exportable flag. Workaround for Boulder issue that doesn\u0026rsquo;t return JSON error bodies for old endpoints. Fixed bug creating new orders with a changed KeyLength value that was preventing the required new private key from being created.  ",
    "ref": "/blog/2019/11/posh-acme-3.11.0/"
  },{
    "title": "Posh-ACME 3.10.0",
    "date": "",
    "description": "Critical fix, new Hurricane Electric plugin, and Azure cert-based auth.",
    "body": "Just shipped a new Posh-ACME release, version 3.10.0. There\u0026rsquo;s a critical fix in this version for a problem introduced by a recent change in Let\u0026rsquo;s Encrypt\u0026rsquo;s ACME implementation that breaks renewals. A new DNS plugin for HurricaneElectric was added and the Azure plugin now supports certificate based authentication in addition to the existing methods. There\u0026rsquo;s also additional guidance in the tutorial on renewals and deployment.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Added new DNS plugin HurricaneElectric Azure plugin now supports certificate based authentication. See the plugin guide for details. (#190) Setup examples in the Azure plugin guide now utilize the Az module rather than the legacy AzureRm.* modules. (#189) Fix for \u0026ldquo;No order for ID\u0026rdquo; errors caused by recent Boulder changes that no longer return order details for expired orders. (#192) Fixed being unable to switch active orders if an error occurred trying to refresh the order details from the ACME server. Added additional guidance on renewals and deployment to the tutorial.  ",
    "ref": "/blog/2019/11/posh-acme-3.10.0/"
  },{
    "title": "Posh-ACME 3.9.0",
    "date": "",
    "description": "New UnoEuro plugin and updates to Cloudflare plugin.",
    "body": "Just shipped a new Posh-ACME release, version 3.9.0. There\u0026rsquo;s a new DNS plugin for UnoEuro thanks to a user submission. The Cloudflare plugin was also updated to support limited use tokens that don\u0026rsquo;t have edit permissions to all zones on an account.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Added new DNS plugin UnoEuro (Thanks @OrKarstoft) Fix for Cloudflare plugin not working properly when limited scope token didn\u0026rsquo;t have at least read permissions to all zones on an account. To use an edit token with limited zone permissions, you must now also specify a secondary token with read permissions to all zones. See the plugin guide for details. (#184) Fix for PropertyNotFound exception when imported plugin data is null or not the expected hashtable value (#182)  ",
    "ref": "/blog/2019/10/posh-acme-3.9.0/"
  },{
    "title": "Posh-IBCLI 1.3.0",
    "date": "",
    "description": "New functions Get-IBCLIApacheCert and Set-IBCLIApacheCert",
    "body": "Just shipped a new Posh-IBCLI release, version 1.3.0. It adds new functions called Get-IBCLIApacheCert and Set-IBCLIApacheCert which wrap the set apache_https_cert command added in NIOS 8.4. What\u0026rsquo;s funny is that the only reason I found out about the command is because of a problem I ran into while testing Posh-IBWAPI\u0026rsquo;s new file upload functions.\nThe file I was trying to upload was a certificate for the web UI and I figured my test procedure would involve uploading my cert and then re-generating a self-signed cert before repeating the test. But the second time I tried to upload a cert, it kept failing with the error, \u0026ldquo;The certificate already exists.\u0026rdquo; After contacting support, I found out that for some reason NIOS keeps copies of all the certs you\u0026rsquo;ve ever imported in the grid database. And until NIOS 8.4, there was no way to re-use an old cert.\nI\u0026rsquo;m still unclear why NIOS is keeping copies of old unused certs, but I really hope the Infoblox engineers expand on the ability to manage them in future versions. At the very least, it\u0026rsquo;d be nice to have a way to delete them and free up database space. Maybe add some WAPI equivalent ways to manage them rather than just the CLI method.\nIn any case, Get-IBCLIApacheCert will give you a list of the certificate serial numbers and common name values that are stored in the database. Set-IBCLIApacheCert will let you set the new active certificate with the specified serial number.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Added Get-IBCLIApacheCert and Set-IBCLIApacheCert to allow manipulation of the web UI certificate on a grid member. These require a CLI command that exists in NIOS 8.4+ and will throw an error on earlier versions.  ",
    "ref": "/blog/2019/10/posh-ibcli-1.3.0/"
  },{
    "title": "Posh-ACME 3.8.0",
    "date": "",
    "description": "More options in Set-PAOrder and misc fixes",
    "body": "Just shipped a new Posh-ACME release, version 3.8.0. Set-PAOrder now supports modifying some order properties that don\u0026rsquo;t require generating a new order such as FriendlyName, PfxPass, and the Install switch. If the order has already been completed, changes to FriendlyName and PfxPass will generate new versions of the associated PFX files with the updated values. But changes to the Install switch will only affect future renewals. The GoDaddy plugin will no longer fail on large accounts with more than 100 domains. There\u0026rsquo;s also a fix for the Cloudflare plugin when used with limited scope tokens.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Set-PAOrder now supports modifying some order properties such as FriendlyName, PfxPass, and the Install switch that don\u0026rsquo;t require generating a new ACME order. FriendlyName or PfxPass changes will regenerate the current PFX files with the new value(s) if they exist. Changes to the Install switch will only affect future renewals. Fixed FriendlyName, PfxPass, and Install parameters not applying when calling New-PACertificate against an existing order (#178) Fixed GoDaddy plugin so it doesn\u0026rsquo;t fail on large accounts (100+ domains) (#179) Updated Cloudflare plugin to workaround API bug with limited scope tokens (#176) Fixed DnsSleep and ValidationTimout being null when manually creating an order with New-PAOrder and finishing it with New-PACertificate. Added parameter help for -NewKey on New-PAOrder which was missing. When using New-PACertificate against an already completed order that is not ready for renewal, the informational message has been changed to Warning from Verbose to make it more apparent that nothing was done. Updated instdev.ps1 so it still works when the BouncyCastle DLL is locked and $ErrorActionPreference is set to Stop. Updated a bunch of plugin guides with info regarding PowerShell 6.2\u0026rsquo;s fix for the SecureString serialization bug and enabling the use of secure parameter sets on non-Windows.  ",
    "ref": "/blog/2019/09/posh-acme-3.8.0/"
  },{
    "title": "Posh-ACME 3.7.0",
    "date": "",
    "description": "New param in Submit-Renewal and misc fixes",
    "body": "Just shipped a new Posh-ACME release, version 3.7.0. In addition to some miscellaneous bug fixes, Submit-Renewal now has an optional -PluginArgs parameter for cases when you need to specify new values for a plugin but don\u0026rsquo;t want to create a whole new order from scratch. This is useful if your credentials change or if the type of credential you\u0026rsquo;re using is purposefully short-lived.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Submit-Renewal now has a PluginArgs parameter to make it easier to update plugin credentials without needing to create a new order from scratch. (Thanks @matt-FFFFFF) The FriendlyName parameter in New-PACertificate and New-PAOrder now defaults to the certificate\u0026rsquo;s primary name instead of an empty string to avoid a Windows bug that can occur when installing the generated PFX files. Fixed Windows plugin issue when using WinZoneScope and not all zones have that scope (#168) Fixed an internal bug with Export-PACertFiles that luckily didn\u0026rsquo;t cause problems due to PowerShell variable scoping rules. Fixed a typo in the Cloudflare guide examples. (Thanks @mccanney)  ",
    "ref": "/blog/2019/09/posh-acme-3.7.0/"
  },{
    "title": "Posh-IBWAPI 3.1.0",
    "date": "",
    "description": "Improvements to Send-IBFile and Receive-IBFile",
    "body": "Just shipped a new Posh-IBWAPI release, version 3.1.0. There is now an -OverrideTransferHost switch in Send-IBFile and Receive-IBFile. But to understand what it does requires a bit of explanation on how file transfers work under the hood with the Infoblox WAPI. Any given file transfer (up or down) is a 3-step process that can be generalized as follows.\n Inform WAPI that you want to do a file transfer and it returns a token and a URL to actually transfer against. Upload or download the file using the URL it gave you. Use the token to inform WAPI the transfer is complete.  With most file transfers, the WAPI host and the URL it wants you to transfer the file against are the same. However, for some reason the URL returned always has the host\u0026rsquo;s IP even if you originally connected via the hostname or FQDN. For example,\n# WAPI URL base \u0026#39;https://gridmaster.example.com/\u0026#39; # Transfer URL base \u0026#39;https://192.168.1.2/\u0026#39; This can be a problem if you\u0026rsquo;re using a trusted SSL\\TLS certificate on your host because they usually don\u0026rsquo;t validate properly for IP addresses. So while the WAPI calls will succeed, the transfer call will fail due to certificate validation. In the previous version, I tried working around this by just always skipping certificate validation on the transfer call. But it turns out that can cause some undesireable behavior in dependent scripts.\nSo what the -OverrideTransferHost actually does is check if the transfer URL\u0026rsquo;s host matches the WAPI URL\u0026rsquo;s host and if it doesn\u0026rsquo;t, tweaks the transfer URL to match. Whatever value is being used for -SkipCertificateCheck is also configured for the transfer call.\nIn addition to the new switch, there are a few other enhancements and fixes in this release. In particular, Send-IBFile will now open the file to be uploaded with a lock that no longer prevents other readers from reading the file simultaneously.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Added OverrideTransferHost switch to Send-IBFile and Receive-IBFile which tweaks the WAPI supplied transfer URL so that the hostname matches the WAPIHost value originally passed to the function. It also copies the state of the SkipCertificate switch to the transfer call. Send-IBFile will no longer lock the file being uploaded so other readers can\u0026rsquo;t read it. Fixed file encoding in Send-IBFile when uploading non-ascii files. Fixed Receive-IBFile on PowerShell Core by working around an upstream bug (#43) Fixed Get-IBObject's ReturnAllFields parameter when not querying the latest WAPI version  ",
    "ref": "/blog/2019/08/posh-ibwapi-3.1.0/"
  },{
    "title": "Posh-ACME 3.6.0",
    "date": "",
    "description": "New plugins and self-hosted http challenges",
    "body": "Just shipped a new Posh-ACME release, version 3.6.0. This one has a bunch of new stuff including new plugins for Domeneshop, Dreamhost, EasyDNS, and freedns.afraid.org. The other big addition is a new function called Invoke-HttpChallengeListener which can be super handy for people doing HTTP challenges. It\u0026rsquo;s basically a self-contained webserver that will respond to requests for the HTTP challenges in your order so you don\u0026rsquo;t have manually deal with making the challenge files available. Check out the wiki page, How To Self Host HTTP Challenges, for an in-depth guide on using it.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Added new DNS plugins  Domeneshop (Thanks @ornulfn) Dreamhost (Thanks @jhendricks123) EasyDNS (Thanks @abrysiuk) FreeDNS (afraid.org)   Added Invoke-HttpChallengeListener function (Thanks @soltroy). This runs a self-hosted web server that can answer HTTP challenges. Look for a wiki usage guide soon. Added Remove-PAServer function. Warning: This deletes all data (accounts, orders, certs) associated with an ACME server. Added Install-PACertificate function. This can be used to manually import a cert to the Windows cert store. (#159) Added support for Cloudflare\u0026rsquo;s new limited access API Tokens. See usage guide for details. Added support for propagation polling with ClouDNS plugin. See usage guide for details. Fixed edge case zone finding bug with ClouDNS plugin. Fixed DOcean (Digital Ocean) plugin which broke because they now enforce a 30 sec TTL minimum on record creation. Fixed overly aggressive error trapping in OVH plugin. (#162) Fixed a typo in the OVH plugin usage guide. Fixed SkipCertificateCheck is no longer ignored when passing a PAServer object via pipeline to Set-PAServer. Fixed Submit-ChallengeValidation no longer tries to sleep when DnsSleep = 0. Some internal refactoring.  ",
    "ref": "/blog/2019/08/posh-acme-3.6.0/"
  },{
    "title": "Auditing Active Directory Passwords With PwnedPassCheck",
    "date": "",
    "description": "Make sure users aren't using compromised passwords.",
    "body": "In a previous post, I introduced a new PowerShell module called PwnedPassCheck. It can be used to check passwords and hashes against a list of over half a billion compromised passwords exposed in data breaches thanks to Troy Hunt\u0026rsquo;s incredibly useful haveibeenpwned.com. In this post, I\u0026rsquo;ll demonstrate how to use the module in conjunction with Michael Grafnetter\u0026rsquo;s amazing DSInternals module to quickly audit existing passwords in Active Directory against the compromised list.\nDoesn\u0026rsquo;t DSInternals Already Support This? \r\r\rYes. Since version 3 of the DSInternals module, the Test-PasswordQuality function has been able to use the monolithic NTLM hash file from Have I Been Pwned as a source for weak password hashes. The function also includes a bunch of other password quality indicators and is a fantastic utility all by itself. It was pretty slow in that version because it was scanning the file line by line and the file is huge. Version 3.3 implemented a binary search option if you had a sorted hash file that was significantly faster.\nPwnedPassCheck has two potential advantanges over DSInternals. The first is the \u0026ldquo;Seen Count\u0026rdquo; which is the number of times a password was seen in breaches. While it could be argued that any password seen in a breach even once is bad and should be changed, I don\u0026rsquo;t think anyone would argue that a password seen over 100,000 times is worse than one only seen once. The other advantage is if you\u0026rsquo;re already hosting or have access to a third party hosting a copy of the NTLM API, you can skip having to deal with downloading the monolithic file to your audit system.\nPrerequisites As I mentioned in the previous post, Have I Been Pwned doesn\u0026rsquo;t currently host an NTLM API version of the data set. So you need to download the NTLM data set file ordered by hash, split the file into the format the API uses, and host it somewhere. Check the previous post for details on that process.\nUltimately, you need to know what to set for the -ApiRoot parameter. Here are some possibilities depending on where you decide to host the files.\n# web server \u0026#39;http://pwned.example.com/range/\u0026#39; # file share \u0026#39;\\\\server.example.com\\share\\hibp-ntlm\\\u0026#39; # local filesystem \u0026#39;C:\\temp\\hibp-ntlm\\\u0026#39; Querying NTHash with DSInternals DSInternals supports two different ways to get AD\u0026rsquo;s NTHash values that you\u0026rsquo;ll be checking; an offline method that reads ntds.dit files and an online method that uses AD\u0026rsquo;s own replication protocol to query a DC directly. In my opinion, the online method is way easier to deal with, so that\u0026rsquo;s what we\u0026rsquo;ll describe here. But if you want or need to go with the offline route, here\u0026rsquo;s a DSInternals blog post describing the process.\nFor the online query method, we\u0026rsquo;ll be using the Get-ADReplAccount function. In order for the query to work, you\u0026rsquo;ll need the Replicating Directory Changes and Replicating Directory Changes All permissions on the domain\u0026rsquo;s root object. By default, members of \u0026ldquo;Domain Admins\u0026rdquo; have these permissions as well as a few other domain controller related groups. But in case you want to setup a dedicated account for this purpose, here\u0026rsquo;s a quick way to add them for a particular group or account.\n$context = (Get-ADRootDSE).defaultNamingContext $userOrGroup = \u0026#39;EXAMPLE\\myuser\u0026#39; \u0026amp;dsacls.exe \u0026#34;$context\u0026#34; /G \u0026#34;$($userOrGroup):CA;Replicating Directory Changes;\u0026#34; \u0026amp;dsacls.exe \u0026#34;$context\u0026#34; /G \u0026#34;$($userOrGroup):CA;Replicating Directory Changes All;\u0026#34; There are a number of different ways to call Get-ADReplAccount. If you want to get every account-like object in the domain (computers, managed service accounts, etc.), you can call it like this.\n$context = (Get-ADRootDSE).defaultNamingContext $dc = (Get-ADDomainController).HostName $accounts = Get-ADReplAccount -All -NamingContext $context -Server $dc Getting everything tends to be overkill for this purpose because things like computers and managed service accounts are highly unlikely to have compromised passwords since they auto-rotate with random long passwords under normal circumstances. Instead, I like to do an initial query with Get-ADUser from the ActiveDirectory PowerShell module. You can use whatever filter and search base options make sense for the audit you want to perform, but here\u0026rsquo;s a basic example.\n$dc = (Get-ADDomainController).HostName $accounts = Get-ADUser -Filter * | Select ObjectGuid | Get-ADReplAccount -Server $dc The Select ObjectGuid portion is important because the parameter binding in Get-ADReplAccount gets confused if you send it the raw output of Get-ADUser. Now you should have a set of \u0026ldquo;DSInternals.Common.Data.DSAccount\u0026rdquo; objects in the $accounts variable that are ready to be sent to PwnedPassCheck.\nChecking The Hashes Each of the DSAccount objects in the $accounts variable you created in the previous step have an NTHash property that contains the hash as a byte array which we can use with Test-PwnedHashBytes. The NTHash property can be null on some of the default domain objects like Guest and DefaultAccount, so we\u0026rsquo;ll filter those out first.\n$accounts | Where-Object { $_.NTHash } | Test-PwnedHashBytes -ApiRoot \u0026#39;http://pwned.example.com/range/\u0026#39; | Select Label,SeenCount The -Label parameter in Test-PwnedHashBytes has an alias of SamAccountName and so parameter binding picks up the value from each DSAccount object automatically and puts it in the output for us. I also purposefully excluded the Hash property from the output because in this case, the Label is likely all you care about. The output might look something like this.\nLabel SeenCount ----- --------- krbtgt 0 Administrator 0 baduser 1177 gooduser 0 So baduser is the only account that had a password found in the compromised data set and it was seen a total of 1177 times which means that user should probably be required to change their password.\nWhat\u0026rsquo;s Next? \r\r\rA one-time audit for compromised passwords is a great idea for any organization who isn\u0026rsquo;t currently doing any password auditing beyond AD\u0026rsquo;s native complexity enforcement. Setting up a regularly scheduled audit that can report to admins and/or take action like forcing a password change on compromised users is an even better idea. Changed passwords since the last audit will be caught and newly compromised passwords will be caught as new versions of the compromised data set are released and you update your local copy.\nIt\u0026rsquo;s also a good idea to prevent compromised passwords from being set it the first place by integrating the check with your existing password change system. How to do this will vary widely on how passwords are currently managed in your organization.\nIf you\u0026rsquo;re using an entirely on-prem Active Directory, there are a variety of free and paid password filter drivers that get installed on your domain controllers and are able to reject passwords that are compromised or don\u0026rsquo;t meet other requirements you configure. A few that I\u0026rsquo;ve run across recently include Lithnet Password Protection, PwnedPasswordsDLL-API, and safepass.me.\nIf your on-prem Active Directory is just a sync\u0026rsquo;d replica from another source or you\u0026rsquo;re using one of the many cloud federated identity providers, you may need to implement it at the source layer. Azure AD\u0026rsquo;s feature called Password Protection is the main one I know about. But I\u0026rsquo;m sure other cloud identity providers have something similar.\nThe NIST password policy guidelines released in 2017 are pretty clear that the conventional wisdom regarding complex passwords and forced rotation were a bad idea. The new guidelines suggest that length is the most important factor in password strength in addition to other caveats like not using dictionary words or repeating characters. And forced rotation should only be necessary in cases of breach or known compromise.\nThanks again to Michael Grafnetter for his incredible work on DSInternals and Troy Hunt for everything related to Have I Been Pwned. You guys are awesome and we\u0026rsquo;re all safer because of your efforts.\n",
    "ref": "/blog/2019/08/auditing-active-directory-passwords-with-pwnedpasscheck/"
  },{
    "title": "New Module: PwnedPassCheck",
    "date": "",
    "description": "Checking if passwords or hashes have been compromised.",
    "body": "Troy Hunt\u0026rsquo;s incredibly useful haveibeenpwned.com is a great way to check whether your email address and other personal information was exposed in a data breach. But it also allows you to separately check if a specific password was exposed in a breach. As of version 5, the data set contains over half a billion compromised passwords and the number of times they\u0026rsquo;ve been seen in data breaches. My PwnedPassCheck module lets you query that data easily via PowerShell.\nThis Sounds Sketchy \r\r\rIf you\u0026rsquo;re thinking that sending your password or even just its hash to a random website on the Internet just to check if it has been compromised seems like a good way to become compromised, you\u0026rsquo;re absolutely right. But since version 2, Troy with the help of Junade Ali from CloudFlare came up with a way to let you query the data safely and still remain anonymous using a model called k-Anonymity.\nEssentially, you never send your password or its full hash to the API server. Instead you send only the first 5 characters of the hash and the server sends back a list of hashes with a matching prefix. You can then check the list locally for the actual result. Not only does this prevent the server operator from getting your password/hash, it\u0026rsquo;s also significantly faster to query than a single multi-gigabyte text file.\nIf you\u0026rsquo;re still unconvinced, you\u0026rsquo;ll be happy to know that this module supports checking against alternative API sources or even filesystem paths. So you can host your own internal copy of the API or even just toss the files on a server somewhere and query that. The official API only supports SHA-1 hashes, so this is actually a requirement if you want to check NTLM hashes unless someone else decides to host a public NTLM version.\nShow Me The Code Already \r\r\rFirst grab the module from the PowerShell gallery. It requires a minimum of PowerShell 3.0, but will happily run cross-platform on PowerShell Core as well. If you\u0026rsquo;re running an ancient version of PowerShell that doesn\u0026rsquo;t include Install-Module, see the readme for alternative instructions.\nInstall-Module PwnedPassCheck Checking Passwords The most basic thing you can do is pass a string to Test-PwnedPassword. Don\u0026rsquo;t use a real password this way because it can be saved in your command history.\nPS C:\\\u0026gt; Test-PwnedPassword \u0026#39;password\u0026#39; Label Hash SeenCount ----- ---- --------- 5BAA61E4C9B93F3F0682250B6CF8331B7EE68FD8 3730471 The output will display the hash of that string along with the \u0026ldquo;seen count\u0026rdquo; which is how many times that password has been seen across all of the data breaches. So popular/insecure passwords like \u0026lsquo;password\u0026rsquo; are inevitably going to have a huge seen count. Ignore the Label column for now.\nWhen you\u0026rsquo;re ready to test a real password, use something like Read-Host or Get-Credential to enter it interactively so it doesn\u0026rsquo;t show up in your command history. Test-PwnedPassword will accept String, SecureString, and PSCredential objects. You can also pass a bunch of them at once via the pipeline.\n$pass = Read-Host -Prompt \u0026#39;Password\u0026#39; $secPass = Read-Host -Prompt \u0026#39;Secure Password\u0026#39; -AsSecureString $cred = Get-Credential # The username in a credential is ignored $pass,$secPass,$cred | Test-PwnedPassword Checking Hashes Test-PwnedPassword is fine when you know the clear text for the passwords you want to check. But for admins who want to audit the passwords of an existing userbase, they (hopefully) only have access to the password hashes. That\u0026rsquo;s where Test-PwnedHash and Test-PwnedHashBytes come in handy.\nThe only restriction on checking hashes at the moment is that they must be either unsalted SHA-1 or NTLM hashes. The official API only supports SHA-1 and both SHA-1 and NTLM versions are available for download to check against locally. It\u0026rsquo;s possible there may be other hash versions in the future, but those are the limitations for now.\nTest-PwnedHash and Test-PwnedHashBytes will accept hashes as a hex-encoded string (case-insensitive) or byte array respectively. If you don\u0026rsquo;t have an existing set of password hashes to use, the module has Get-SHA1Hash and Get-NTLMHash helper functions you can test with.\n$hashHex = Get-SHA1Hash \u0026#39;P@ssword1\u0026#39; Test-PwnedHash $hashHex $hashBytes = Get-SHA1Hash \u0026#39;P@ssword1\u0026#39; -AsBytes Test-PwnedHashBytes $hashBytes Querying Locally and NTLM For those who can\u0026rsquo;t or won\u0026rsquo;t send even the first 5 characters of a password hash to a third party website, it\u0026rsquo;s fairly easy to host your own local copy. This is also necessary if you want to check NTLM hashes because the official API only supports SHA-1.\nThe Pwned Passwords page has links to download the 7-zip compressed files. Download the one ordered by hash for hash type you want to query against and extract it. The version 5 files are roughly 20 GB once extracted so make sure you have enough disk space.\nSplitting The File This is the largest drawback to querying locally. But you only have to do it once. 5 hexadecimal characters have a total of 16^5 (1,048,576) combinations which makes splitting the file by hand impractical. Each line is in the form \u0026lt;hash\u0026gt;:\u0026lt;count\u0026gt; except the occasional line that has the string \u0026ldquo;NULL\u0026rdquo;. So for each line you need to make sure it\u0026rsquo;s not NULL, split the line into its prefix (first 5 characters) and suffix (the rest of the line), then append the suffix to a file named with the prefix. It\u0026rsquo;s not a super difficult scripting exercise, but the size of the original file and the quantity of resulting files makes it a fairly storage I/O intensive process. So if your disk is slow, it may take a while.\nI\u0026rsquo;ve added a Split-Scripts folder to the module\u0026rsquo;s GitHub repository that contains a couple scripts I used to split the file for my own purposes on various systems. If you end up writing your own script that would be useful to others, feel free to contribute to the project and send a pull request.\nHosting The Files When you finish splitting the file, you should have a folder with 1,048,576 files named 00000 through FFFFF that are each roughly 20 KB in size. You can either leave them on the filesystem if you\u0026rsquo;re doing the checks from the same system, move them to a file share, or host them on an internal web server. If they\u0026rsquo;re on a web server, make sure it\u0026rsquo;s configured to serve the files as text/plain MIME type. You should be able to go to a URL like http://pwned.example.com/range/00000 in a browser and see the contents of the file.\nRunning Local Queries All of the Test-* functions take an optional -ApiRoot parameter that will override the default pwnedpasswords.com URL. The parameter will accept URLs or filesystem/UNC paths. So any of the following examples would work. Just make sure to include the entire path except for the 5 character hash prefix.\n# These work the same way using Test-PwnedHash and Test-PwnedHashBytes Test-PwnedPassword \u0026#39;password\u0026#39; -ApiRoot \u0026#39;C:\\temp\\pwned\\\u0026#39; Test-PwnedPassword \u0026#39;password\u0026#39; -ApiRoot \u0026#39;\\\\server\\share\\pwned\\\u0026#39; Test-PwnedPassword \u0026#39;password\u0026#39; -ApiRoot \u0026#39;http://pwned.example.com/range/\u0026#39; Querying NTLM For NTLM API sources, you\u0026rsquo;ll need to set the -HashType NTLM parameter with Test-PwnedPassword in addition to the -ApiRoot parameter. The Test-PwnedHash* functions assume the hash is already of the correct type. So no change is needed for those other than setting -ApiRoot.\nTest-PwnedPassword \u0026#39;password\u0026#39; -ApiRoot \u0026#39;C:\\temp\\pwned\\\u0026#39; -HashType \u0026#39;NTLM\u0026#39; Get-NTLMHash \u0026#39;password\u0026#39; | Test-PwnedHash -ApiRoot \u0026#39;http://pwned.example.com/range/\u0026#39; Wrapping Up Huge thanks to Troy Hunt for his tireless work maintaining haveibeenpwned.com and the rest of his contributions to the larger security community making people safer online. I hope this module helps add to that in some small way. I\u0026rsquo;m also planning on adding another post soon that demonstrates how to easily do a password audit against Active Directory using this module, so be on the lookout for that.\n",
    "ref": "/blog/2019/08/new-module-pwnedpasscheck/"
  },{
    "title": "Workaround For AD PSDrive Bug In Server 2019",
    "date": "",
    "description": "PSPath to the rescue",
    "body": "TL;DR Until Microsoft fixes the bug with the AD PSDrive provider, use the fully qualified PSPath to the object instead of the \u0026ldquo;AD:\u0026rdquo; PSDrive path like this:\n$DN = (Get-ADUser jdoe).distinguishedName $objectPath = \u0026#34;Microsoft.ActiveDirectory.Management.dll\\ActiveDirectory:://RootDSE/$DN\u0026#34; Get-Acl -Path $objectPath The Story I was browsing r/PowerShell recently and came across a thread from someone who had run into a PowerShell bug after upgrading to Windows Server 2019 (1809). The user was attempting to run Get-Acl against an AD object using the object\u0026rsquo;s distinguished name like this:\n$DN = (Get-ADUser jdoe).distinguishedName $ACL = Get-Acl -Path \u0026#34;AD:\\$DN\u0026#34; The code works just fine in Server 2016 and earlier. But the same code throws an error in Server 2019 and reportedly Windows 10 (1903) as well.\nGet-Acl : The object name has bad syntax At line:1 char:8 + $ACL = Get-Acl -Path \u0026#34;AD:\\$DN\u0026#34; + ~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : NotSpecified: (\\\\RootDSE\\CN=Do...C=example,DC=com:String) [Get-Acl], ADException + FullyQualifiedErrorId : ADProvider:ItemExists::ADError,Microsoft.PowerShell.Commands.GetAclCommand The problem is this particular user\u0026rsquo;s DN had an escaped comma in it because the CN value was set to \u0026ldquo;Doe, Jane\u0026rdquo;. Commas are allowed in object names in AD, but they tend to cause all sorts of problems with LDAP path parsing code due to the escaping. The resulting DN of the object looks something like this:\nCN=Doe\\, Jane,CN=Users,DC=example,DC=com Historically, the Windows native Active Directory PowerShell cmdlets and PSDrive provider have no issues dealing with this. But apparently a bug crept into the latest releases. It should be noted that the bug exists with any cmdlet that depends on the Active Directory PSDrive provider. Get-Acl just happens to be the one this user was using. But it also affects something as simple as Get-Item.\nThe Workaround One of the things I tried while poking around this bug looking for a workaround was running Get-ChildItem against the parent container and then running Get-Acl against the PSPath parameter of the results like this.\nGet-ChildItem \u0026#34;CN=Users,DC=example,DC=com\u0026#34; | ForEach-Object { Get-Acl -Path $_.PSPath } To my pleasant surprise, it worked without issue. So I took a look at the format of the PSPath for these objects and found this for my problematic test user.\nMicrosoft.ActiveDirectory.Management.dll\\ActiveDirectory:://RootDSE/CN=Doe\\, Jane,CN=Users,DC=example,DC=com We can generalize this out to a somewhat ugly (but at least usable) path prefix for any DN and work it into the original code like this.\n$DN = (Get-ADUser jdoe).distinguishedName $objectPath = \u0026#34;Microsoft.ActiveDirectory.Management.dll\\ActiveDirectory:://RootDSE/$DN\u0026#34; $ACL = Get-Acl -Path $objectPath ",
    "ref": "/blog/2019/07/workaround-for-ad-psdrive-bug-in-server-2019/"
  },{
    "title": "Infoblox and MS Management Permissions",
    "date": "",
    "description": "How to avoid using Domain Admin",
    "body": "The NIOS documentation lacks great instructions for granting least-privilege access to use the various MS Management components. As a former Active Directory admin, that bugs me because people get frustrated and end up giving service accounts Domain Admin permissions just to get things working. This post will lay out the necessary permissions for each component and provide PowerShell examples on how to apply them easily.\nWarning: I\u0026rsquo;ve tested these permissions and examples against NIOS 8.4 and Windows Server 2008 R2 through 2019. Your mileage may vary if your environment doesn\u0026rsquo;t fit those characteristics.\nPrerequisites DNS and DHCP management don\u0026rsquo;t require targetting domain controllers or even domain joined servers. But it is the most common configuration so our examples will reflect that assumption. If you\u0026rsquo;re targetting a non-domain joined server, you\u0026rsquo;ll need to change the user/group references accordingly and modify the PowerShell commands to use their non-AD equivalents.\nInfoblox allows you to use a separate service account for each component, but most organizations choose to use one account for simplicity. So we\u0026rsquo;ll do the same here with a standard AD user account called svc-infoblox.\nMost of these components have the choice to be configured in \u0026ldquo;Read-only\u0026rdquo; or \u0026ldquo;Read/Write\u0026rdquo; mode. For organizations who want to use the features in Read-only mode, I still usually suggest granting the permissions necessary for Read/Write in case they change their mind later. Ironically, it also tends to be harder to grant read-only permissions for these services. The examples here will assume Read/Write mode.\nIn my testing, no manual changes to the Windows default firewall settings were needed. However, if you have group policy or external firewalls locking things down, you\u0026rsquo;ll likely need to open things like the dynamic RPC port range between the managing grid member and the target server. See the Deployment Guidelines doc page for details.\nDNS \r\rDNS Toggleable Options\r\rSynchronize Data This is the core ability to synchronize zone and record data. All we need is membership in DnsAdmins.\nGet-ADGroup \u0026#39;DnsAdmins\u0026#39; | Add-ADGroupMember -Members (Get-ADUser \u0026#39;svc-infoblox\u0026#39;) Monitor and control DNS Services This adds the ability to see the status of and start/stop the DNS service on the target machine. It requires full control on the local service control manager and DNS service. We\u0026rsquo;ll give the permissions to the DnsAdmins group because our service account is already a member.\nMake sure to run these on the target server.\n# Full control on SCManager for DnsAdmins $scmSDDL = (\u0026amp;sc.exe sdshow scmanager)[1] $DnsAdminsSID = (Get-ADGroup DnsAdmins).SID.ToString() $newSDDL = $scmSDDL.Insert(2, \u0026#34;(A;;KA;;;$DnsAdminsSID)\u0026#34;) \u0026amp;sc.exe sdset scmanager $newSDDL # Full control on on DNS service for DnsAdmins $dnsSDDL = (\u0026amp;sc.exe sdshow dns)[1] $DnsAdminsSID = (Get-ADGroup DnsAdmins).SID.ToString() $newSDDL = $dnsSDDL.Insert(2, \u0026#34;(A;;CCDCLCSWRPWPDTLOCRSDRCWDWO;;;$DnsAdminsSID)\u0026#34;) \u0026amp;sc.exe sdset dns $newSDDL Windows Server 2016 and later adds an additional hurdle for controlling services from a non-admin account as described in KB 4457739. We need to exempt the DNS service from the new protection.\n$keyName = \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurePipeServers\\SCM\u0026#39; $valName = \u0026#39;RemoteAccessCheckExemptionList\u0026#39; $key = Get-Item $keyName $values = $key.GetValue($valName) if (\u0026#39;dns\u0026#39; -notin $values) { $values += \u0026#39;dns\u0026#39; Set-ItemProperty $keyName $valName $values -Type MultiString } Synchronize DNS Reporting Data Important: This feature is only compatible with Windows Server 2012 R2 with KB 2919355 or Server 2016 and later. So make sure it\u0026rsquo;s disabled on anything earlier. See the \u0026ldquo;Synchronizing DNS Reporting Data\u0026rdquo; section in the docs for more details.\nWe need the ability to read the DNS specific event logs. So we\u0026rsquo;ll add the service account to Event Log Readers.\nGet-ADGroup \u0026#39;Event Log Readers\u0026#39; | Add-ADGroupMember -Members (Get-ADUser \u0026#39;svc-infoblox\u0026#39;) DHCP \r\rDHCP Toggleable Options\r\rSynchronize Data This is the core ability to synchronize the scope data. All we need is membership in DHCP Administrators.\nGet-ADGroup \u0026#39;DHCP Administrators\u0026#39; | Add-ADGroupMember -Members (Get-ADUser \u0026#39;svc-infoblox\u0026#39;) Monitor and control DHCP Services This adds the ability to see the status of and start/stop the DHCP service on the target machine. It requires full control on the local service control manager and DHCP service. We\u0026rsquo;ll give the permissions to the DHCP Administrators group because our service account is already a member.\nMake sure to run these on the target server.\n# Full control on SCManager for DHCP Administrators $scmSDDL = (\u0026amp;sc.exe sdshow scmanager)[1] $DhcpAdminsSID = (Get-ADGroup \u0026#39;DHCP Administrators\u0026#39;).SID.ToString() $newSDDL = $scmSDDL.Insert(2, \u0026#34;(A;;KA;;;$DhcpAdminsSID)\u0026#34;) \u0026amp;sc.exe sdset scmanager $newSDDL # Full control on on DNS service for DnsAdmins $dhcpSDDL = (\u0026amp;sc.exe sdshow dhcpserver)[1] $DhcpAdminsSID = (Get-ADGroup \u0026#39;DHCP Administrators\u0026#39;).SID.ToString() $newSDDL = $dhcpSDDL.Insert(2, \u0026#34;(A;;CCDCLCSWRPWPDTLOCRSDRCWDWO;;;$DhcpAdminsSID)\u0026#34;) \u0026amp;sc.exe sdset dhcpserver $newSDDL Windows Server 2016 and later adds an additional hurdle for controlling services from a non-admin account as described in KB 4457739. We need to exempt the DHCP service from the new protection.\n$keyName = \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurePipeServers\\SCM\u0026#39; $valName = \u0026#39;RemoteAccessCheckExemptionList\u0026#39; $key = Get-Item $keyName $values = $key.GetValue($valName) if (\u0026#39;dhcpserver\u0026#39; -notin $values) { $values += \u0026#39;dhcpserver\u0026#39; Set-ItemProperty $keyName $valName $values -Type MultiString } Active Directory Sites \r\rActive Directory Sites Basic Options\r\rIf you plan to use LDAP over SSL (LDAPS), make sure your domain controllers have valid certificates. If the certificates are from an internal PKI infrastructure, make sure the CA certificate has been uploaded to Infoblox as well.\n\r\rActive Directory Sites Basic Options\r\rMake sure \u0026ldquo;Default IP site link\u0026rdquo; is set to a real site link in your environment even if you don\u0026rsquo;t plan to create sites directly from Infoblox.\nImportant: Infoblox will only synchronize sites/subnets with the DC holding the PDC emulator (PDCe) FSMO role. In a multi-domain forest, you should only target the PDCe in the root domain. Some organizations also choose to configure a second DC which FSMO roles would be transferred to in a DR situation. Infoblox will simply ignore it until it becomes the PDCe.\nPermissions for Subnets These permissions will allow Infoblox to assign/unassign subnets to/from existing sites.\nCN=Subnets,CN=Sites,\u0026lt;Configuration Naming Context\u0026gt;\n Applies to: This object only  Create Subnet objects Delete Subnet objects   Applies to: Descendant subnet objects  Read all properties Write all properties Delete Delete subtree    PowerShell example:\n$cfgContext = (Get-ADRootDSE).configurationNamingContext $target = \u0026#34;AD\\svc-infoblox\u0026#34; # group or user reference \u0026amp;dsacls.exe \u0026#34;CN=Subnets,CN=Sites,$cfgContext\u0026#34; /G \u0026#34;$($target):CCDC;subnet\u0026#34; \u0026amp;dsacls.exe \u0026#34;CN=Subnets,CN=Sites,$cfgContext\u0026#34; /I:S /G \u0026#34;$($target):RPWPSDDT;;subnet\u0026#34; Permissions for Sites These permissions will allow Infoblox to create/delete/rename site objects. On creation, the new site will be associated with the \u0026ldquo;Default IP site link\u0026rdquo; mentioned earlier. In these examples, we\u0026rsquo;ll assume that is DEFAULTIPSITELINK.\nCN=Sites,\u0026lt;Configuration Context\u0026gt;\n Applies to: This object only  Create Site objects Delete Site objects   Applies to: Descendant Site objects  Read all properties Write all properties Delete Delete subtree Create all child objects    CN=\u0026lt;Default SiteLink\u0026gt;CN=IP,CN=Inter-Site Transports,CN=Sites,\u0026lt;Configuration Context\u0026gt;\n Read/Write siteList  PowerShell example:\n$cfgContext = (Get-ADRootDSE).configurationNamingContext $target = \u0026#34;AD\\svc-infoblox\u0026#34; # group or user reference $defaultSiteLink = \u0026#34;CN=DEFAULTIPSITELINK,CN=IP,CN=Inter-Site Transports,CN=Sites,$cfgContext\u0026#34; \u0026amp;dsacls.exe \u0026#34;CN=Sites,$cfgContext\u0026#34; /G \u0026#34;$($target):CCDC;site\u0026#34; \u0026amp;dsacls.exe \u0026#34;CN=Sites,$cfgContext\u0026#34; /I:S /G \u0026#34;$($target):RPWPSDDTCCLCRC;;site\u0026#34; \u0026amp;dsacls.exe $defaultSiteLink /G \u0026#34;$($target):RPWP;siteList\u0026#34; Network Users \r\rNetwork Users Options\r\rFor this, Infoblox is just reading the Security event log on the target domain controller. So we need membership in Event Log Readers.\nGet-ADGroup \u0026#39;Event Log Readers\u0026#39; | Add-ADGroupMember -Members (Get-ADUser \u0026#39;svc-infoblox\u0026#39;) ",
    "ref": "/blog/2019/06/infoblox-and-ms-management-permissions/"
  },{
    "title": "Posh-ACME 3.5.0",
    "date": "",
    "description": "New plugin and misc fixes",
    "body": "Just shipped a new Posh-ACME release, version 3.5.0. This one has an important fix due to a recent change in Let\u0026rsquo;s Encrypt\u0026rsquo;s ACME implementation which now more strictly adheres to the recently finalized RFC 8555 spec. The Let\u0026rsquo;s Encrypt change is only on the staging server at the moment, but it will likely move to production soon and it breaks account creation on all previous versions of Posh-ACME.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Added new DNS plugin for Simple DNS Plus (#149) (Thanks @alphaz18) Changed a bunch of \u0026ldquo;-ErrorAction SilentlyContinue\u0026rdquo; references to \u0026ldquo;Ignore\u0026rdquo; so we\u0026rsquo;re not filling the $Error collection with junk. Fix for Boulder removing ID field from new account output. Fixed an issue in a number of plugins that could cause errors if the case of the requested record didn\u0026rsquo;t match the server\u0026rsquo;s zone case. (Thanks @Makr91) Fixed a bug with the Route53 plugin when used on PowerShell Core without the AwsPowerShell module installed. Fixed some typos in the OVH plugin usage guide examples (#147)  ",
    "ref": "/blog/2019/06/posh-acme-3.5.0/"
  },{
    "title": "Posh-IBWAPI 3.0.0",
    "date": "",
    "description": "File upload/download wrappers, named configs, and auto session management",
    "body": "Just shipped a new Posh-IBWAPI release, version 3.0.0. It has only been two days since 2.0.0, but I goofed and shipped 2.0.0 with some breaking changes that I ended up reverting. The biggest feature of the release is the new file upload/download wrappers, Send-IBFile and Recieve-IBFile. They allow you to more easily do things like upload a certificate or download a grid backup and fill a functionality gap that has been bugging me since 1.0.\nFor example, in 1.x this is how you\u0026rsquo;d download a grid backup:\n# request a download token and URL $dl = Invoke-IBFunction -ref fileop -name getgriddata -args @{type=\u0026#39;BACKUP\u0026#39;} # download the file Invoke-IBWAPI -Uri $dl.url -ContentType \u0026#39;application/force-download\u0026#39; ` -Credential (Get-IBWAPIConfig).Credential ` -OutFile .\\backup.tar.gz # inform Infoblox that the download is complete Invoke-IBFunction -ref fileop -name downloadcomplete -args @{token=$dl.token} But in 3.x, you can now do this:\nReceive-IBFile getgriddata -args @{type=\u0026#39;BACKUP\u0026#39;} -OutFile .\\backup.tar.gz Also in the release are named configs which enable saving different credentials for the same grid. The module takes care of session management now as well. So you don\u0026rsquo;t need to manually deal with WebSession objects to prevent extraneous authentication requests.\nUpdated versions can be found in the PowerShell Gallery or GitHub. Installation instructions are in the Readme.\nChangelog  Breaking Changes  The change to ObjectType parameter in Invoke-IBFunction has been reverted to ObjectRef like in 1.x. I totally confused myself during 2.x development of the *-IBFile functions and thought it had been wrong the whole time. It seems silly to do another major version change after two days. But breaking changes demand it according to semver. The ObjectType parameter in Send-IBFile and Receive-IBFile have been changed to ObjectRef to match Invoke-IBFunction. Both still default to \u0026lsquo;fileop\u0026rsquo; and have parameter aliases for \u0026lsquo;type\u0026rsquo; and \u0026lsquo;ObjectType\u0026rsquo; to maintain compatibility with the short lived 2.x codebase.   Fixed example in Invoke-IBFunction help.  And here\u0026rsquo;s the changelog for 2.0.0 which gives a better idea of what has changed since 1.x:\n Breaking Changes  .NET 4.5+ is now required on PowerShell Desktop edition for full functionality. A warning will be thrown when loading the module if it is not found. The WebSession parameter has been removed from all functions except Invoke-IBWAPI. Session handling is now automatic. New-IBSession has been removed. Get-IBWAPIConfig, Set-IBWAPIConfig, and Remove-IBWAPIConfig have been renamed to Get-IBConfig, Set-IBConfig, and Remove-IBConfig respectively. Save-IBWAPIConfig has been removed. Configs are now saved by default via Set-IBConfig. Configs are now referenced by a ProfileName. Old 1.x configs will be automatically backed up, converted, and the new profiles will have their WAPIHost value set as the initial profile name. Set-IBConfig now has ProfileName as its first parameter. Get-IBConfig and Remove-IBConfig now have ProfileName instead of WAPIHost as their selection parameter. The IgnoreCertificateValidation switch has been renamed to SkipCertificateCheck in all functions and configs to be more in line with PowerShell Core. The ObjectRef parameter in Invoke-IBFunction has been changed to ObjectType which is functionally how it always worked and was inappropriately named. Functions get called against object types not references.   New Feature: Automatic session handling. The module will now automatically save and use WebSession objects to increase authentication efficiency over multiple requests and function calls. New Feature: Named configuration profiles. This will allow you to save multiple profiles for the same WAPI host with different credentials, WAPI versions, etc. New functions Send-IBFile and Recieve-IBFile which are convenient wrappers around the fileop functions. See the cmdlet help or the guide in the wiki for more details. Config profiles are now automatically saved to disk when using Set-IBConfig. Set-IBConfig now has a NewName parameter to rename the profile. Get-IBConfig now returns a typed object with a automatically styled display. Remove-IBConfig now has pipeline support both by value and property name so you can pipe the output of Get-IBConfig to it. Get-IBConfig, Set-IBConfig, and Remove-IBConfig now have tab completion on PowerShell 5.0 or later.  ",
    "ref": "/blog/2019/04/posh-ibwapi-3.0.0/"
  }]
